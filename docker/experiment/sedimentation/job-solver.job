#!/bin/bash

#SBATCH -J sedimentation      # Job name
#SBATCH -o log.%j.out         # Name of stdout output file (%j expands to jobId)
#SBATCH -e log.%j.err         # Name of stderr output file (%j expands to jobId)
#SBATCH -p development        # Queue name
#SBATCH -N 4                  # Total number of nodes requested (16 cores/node)
#SBATCH -n 64                 # Total number of mpi tasks requested
#SBATCH -t 02:00:00           # Run time (hh:mm:ss)

# Launch application
echo "Starting job..."

echo "Configuring workspace..."
export CPATH=/work/03664/silva/experiments/sedimentation
export DATAPATH=$CPATH/data
export DATAFLOW_TAG=sedimentation
cd $CPATH

#echo "Specifying dataflow generation..."
#./pg-dataflow.sh

echo "Configuring the lists of machines for the solver and database system..."
echo $SLURM_NODELIST > nodes.txt
java -jar /work/03664/silva/experiments/stampede/EnvironmentFileGenerator-Stampede.jar nodes.txt

echo "Starting database system..."
cd $CPATH
rm -rf data
unzip /work/03664/silva/scripts/monetdb/monetdb-database.zip
/work/03664/silva/experiments/stampede/database_starter.sh database.conf $CPATH $DATAPATH
sleep 15

echo "Executing sedimentation solver..."
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/work/03664/silva/programs/paraview/lib/paraview-5.0
#/usr/bin/time ibrun -n 32 -o 16 /work/03664/silva/experiments/libmesh-sedimentation/libmesh-sedimentation-opt analysis2D.py
#/usr/bin/time ibrun -n 240 -o 16 /work/03664/silva/experiments/libmesh-sedimentation/libmesh-sedimentation-opt analysis3D.py
/usr/bin/time ibrun -n 48 -o 16 /work/03664/silva/experiments/libmesh-sedimentation/libmesh-sedimentation-opt analysis3D.py

echo "Calculating total elapsed times based on log files..."
java -jar /work/03664/silva/experiments/dfa/ProvenanceAnalysis.jar $CPATH/prov

echo "Executing termination algorithm..."
/work/03664/silva/experiments/stampede/termination.sh $CPATH $DATAFLOW_TAG

#echo "Backing up provenance database..."
#mclient -p 50000 -d dataflow_analyzer --dump > prov-db.dump

echo "Finishing job..."
